{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to XSC Learning Notes","text":"<p>The static website is used for personal learning notes.</p>"},{"location":"#some-markdown-skill","title":"Some Markdown Skill","text":""},{"location":"#code-block-and-inline-code","title":"code block and inline code","text":"<pre><code>print(\"Hello World\")\n</code></pre> <p>This is <code>inline</code> code.</p>"},{"location":"#link-and-footnotes","title":"link and footnotes","text":"<p>Hyperlink used to switch to a <code>correct website</code>.</p> <p>We can also link to <code>somewhere</code> of the context.</p> <p>Setting footnotes<sup>1</sup> is similar to the operation of link. </p>"},{"location":"#group-content","title":"group content","text":"<p>Text and code can be grouped.</p> codetext <pre><code>printf(\"Hello World\");\n</code></pre> <ul> <li>STEP1: ...</li> <li>STEP2: ...</li> <li>STEP3: ...</li> </ul>"},{"location":"#table","title":"table","text":"<p>Create a table as following.</p> Object Description A ... B ... C ..."},{"location":"#grid","title":"grid","text":"<p>Create grids as following.</p> <ul> <li>Inroduction</li> <li>Relative Work</li> <li>Experiment</li> <li>Conclusion</li> </ul>"},{"location":"#image","title":"image","text":"<p>Image cam be added by HTML.</p> From WebFrom Local <p> From Web </p> <p> From Local </p>"},{"location":"#math","title":"math","text":"<p>Syntax block as Following.</p> \\[ \\int^{\\pi}_0 sinx dx = 2 \\] <p>Inline syntax as \\(sinx = \\lim_{n \\to \\infty}\\sum^n_0 (-1)^{n}\\frac{1}{(2n+1)!} x^{2n+1}\\)</p>"},{"location":"#emoji","title":"emoji","text":"<p>Search the emoji from the <code>database</code>.</p> <p> </p>"},{"location":"#to-be-added","title":"To Be Added","text":"<ol> <li> <p>Here can add some footnotes.\u00a0\u21a9</p> </li> </ol>"},{"location":"Paper-Reading/","title":"Paper Reading","text":"<p>This page is used for note taking when reading papers.</p> <p><code>summary</code> and my own <code>understanding</code></p>"},{"location":"Paper-Reading/#link-to-jump","title":"Link To Jump","text":"<ul> <li> <p><code>On_orbit Accelerator</code>: Do some research about embadded devices (mainly FPGA) for on_orbit neural network accelerating application. </p> </li> <li> <p><code>Chp2</code>: ...</p> </li> <li> <p><code>Chp3</code>: ...</p> </li> </ul>"},{"location":"Paper-Reading/On_orbit-Accelerator/","title":"On_orbit Accelerator","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#soc-fpga-acceleration-for-semantic-segmentation-of-clouds-in-satellite-images","title":"<code>SoC FPGA Acceleration for Semantic Segmentation of Clouds in Satellite Images</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#summary","title":"Summary","text":"<p><code>\u536b\u661f\u56fe\u50cf\u8bed\u4e49\u5206\u5272</code></p> <ul> <li>Network: Lightweight Dilation U-Net</li> <li>Dataset: 95-Cloud Dataset</li> <li>Device: Xilinx MPSoC FPGA</li> <li>Platform: Vitis AI framework</li> </ul> <p><code>trade-off</code></p> <ul> <li>accuracy</li> <li>execution time &amp; hardware resources</li> </ul> <p><code>LD-UNet</code> LD-UNet\u662fUNet\u7684\u7b80\u5316\u7248\uff0c\u51cf\u5c11computational cost\u548cmemory footprint\uff0c\u540c\u65f6\u4fdd\u6301competitive performance.</p> LD-UNet UNet downsampling and upsampling 2 4 convolution Dilation Residual Block standard <p> </p>  The LD-UNet CNN model  <p> </p>  The custom Dilation Residual Block  <p>The first DSC involves a dilated depthwise convolution with a 3\u00d73 kernel, dilation rate of 2 and stride 1 to increase the receptive field of the convolution. The second DSC involves a depthwise convolution with a 1\u00d71 kernel, acting as the identity shortcut of the residual block.</p> <p>Compare with other model -&gt; best trade-off between param &amp; accuracy</p> <p> </p>  Performance comparision  <p><code>FPGA architecture</code></p> <p>ZYNQ\u5206\u4e3a\u4e24\u5927\u90e8\u5206\u2014\u2014PS\u548cPL\uff1a</p> <ul> <li>PS\u2014\u2014post-processing</li> <li>PL\u2014\u2014pre-processing(kernal) &amp; inference(DPU)</li> </ul> <p> </p>  The FPGA processing architecture  <p><code>result</code></p> <p> </p>  Resources utilization  <p> </p>  Execution time"},{"location":"Paper-Reading/On_orbit-Accelerator/#cloudsatnet-1-fpga-based-hardware-accelerated-quantized-cnn-for-satellite-on-board-cloud-coverage-classification","title":"<code>CloudSatNet-1: FPGA-Based Hardware-Accelerated Quantized CNN for Satellite On-Board Cloud Coverage Classification</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#summary_1","title":"Summary","text":"<p><code>\u4e91\u5c42\u8986\u76d6\u5206\u7c7bCNN\u7684FPGA\u52a0\u901f</code></p> <ul> <li>Network: CloudSatNet-1</li> <li>Dataset: L8 biome Dataset</li> <li>Device: Xilinx Zynq-7020</li> </ul> <p>Why cloud coverage classification?</p> <p>satellite data   downlink resourcecs tight  on-board classification</p> <p>\u901a\u8fc7\u5206\u7c7b\u4e22\u5f03\u88ab\u4e91\u8986\u76d6\u7684\u536b\u661f\u56fe\u50cf\uff0c\u8282\u7701\u4f20\u8f93\u5e26\u5bbd</p> <p><code>method</code></p> <p>The current methods for cloud coverage estimation or classification are mainly categorized into traditional and machine-learning-based approaches</p> Threshold-based (fixed or adaptive) methodTime differentiationStatistical methods <p>Rely on a visible reflection and infrared temperature of the clouds  performance weakens on low-contrasted images</p> <p>Do not consider changes in the top of atmosphere reflectance affected by floods.</p> <p>Combine spectral and spatial features extracted from RSIs with classical machine learning algorithms (support vector machine, decision tree), but they lack to obtain the desired results</p> <p><code>neural network</code></p> <p> </p>  CloudSatNet-1 architecture  <p>About quantization</p> <ul> <li>effect: param   accuracy   power consumption   execution time </li> <li>weight &amp; activations quantization\u5bf9 \u7f51\u7edc\u6027\u80fd \u5f71\u54cd\u4e0d\u5927\uff0c\u4f46\u53ef\u4ee5\u51cf\u5c0f memory footprint \u65b9\u4fbf\u5728FPGA\u4e0a\u7684 \u90e8\u7f72 .</li> </ul> <p>\u4e3b\u8981\u8fdb\u884c4-bit\u91cf\u5316\uff0c\u5934\u5c3e\u4e24\u5c42\u5bf9\u91cf\u5316\u66f4\u654f\u611f\uff0c\u56e0\u6b64\u91c7\u75288-bit\u91cf\u5316</p> <p><code>workflow</code></p> <p> </p>   Scheme of proposed workflow  Quantization Step<pre><code>1. floating point model -&gt; quantized onnx model   with QAT\n2. quantized onnx model -&gt; HLS code   with FINN framework\n3. HLS code -&gt; bit file    with Vivado\n4. deploy on FPGA\n</code></pre> <p><code>result</code></p> Metrics for different BW (including snow/ice)Relationship between ACC and FPRMetrics while deploying on FPGAResource utilization <p>  Results of cloud coverage classification for best-performed models  </p> <p>  Dependence of model ACC on inverse FPR value  </p> <p>  Results of cloud coverage classification for best-performed quantized models on FPGA  </p> <p>  FPGA resource utilization and performance for different model bit widths and folding setup  </p> <p>Tip</p> <p>\u90e8\u7f72\u5728FPGA\u4e0a\u53ea\u67094/3/2 bit model\uff0c\u56e0\u4e3a32 bit\u5360\u7528\u8d44\u6e90\u8fc7\u591a\u65e0\u6cd5\u90e8\u7f72.</p> <p>snow/ice\u7c7b\u522b\u4e0ecloud coverage\u6bd4\u8f83\u50cf\uff0c\u96be\u4ee5\u6709\u6548\u8bc6\u522b.</p> Snow/Ice without cloud coverageWater with cloud coverageMetrics for different biomes <p>  Snow/Ice  </p> <p>  Water  </p> <p>  Results of cloud coverage classification for best-performed 4-bit width models per biome  </p> <p>Try to exclude snow/ice</p> <p>\u5c06snow/ice\u7c7b\u522b\u6392\u9664\u8fdb\u884c\u6d4b\u8bd5\uff0c\u8bc6\u522b\u65b0\u80fd\u663e\u8457\u63d0\u5347.</p> <p><code>flaw</code></p> <ul> <li>High value of FPR on cloud-like feature (e.g. snow/ice).     multi-spectral bands</li> <li>Original L8 biome dataset were merged to form a binary problem.</li> <li>Did not cover the effects of the radiation on the cloud detection system.</li> </ul>"},{"location":"Paper-Reading/On_orbit-Accelerator/#question","title":"Question","text":"<p>\u5b81\u613f\u591a\u5904\u7406\u8d1f\u6837\u672c\u4e5f\u4e0d\u613f\u610f\u4e22\u5f03\u6b63\u6837\u672c\uff0c\u4e0d\u662f\u5e94\u8be5\u4f7fFNR(False Negative Rate)\u66f4\u4f4e\u5417?</p> <p>However, rather than the highest overall accuracy, this study emphasizes on the low FPR (it is more convenient to process a redundant image than to discard the relevant one).</p> <p>\u6ca1\u6709\u5177\u4f53\u8bf4\u600e\u4e48\u63a7\u5236network throughput. </p> <p>Based on the estimated number of cycles per layer reported in Table 3, it is visible that a bottle-neck in the first layer limited the optimal throughput, and it would require a change in the network architecture to allow a higher throughput target. It was demonstrated that the network throughput can be controlled to target a specific FPS desired by the needs of the mission.</p>"},{"location":"Paper-Reading/On_orbit-Accelerator/#fpga-based-implementation-of-ship-detection-for-satellite-on-board-processing","title":"<code>FPGA-Based Implementation of Ship Detection for Satellite On-Board Processing</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#summary_2","title":"Summary","text":"<p><code>\u536b\u661f\u5728\u8f68\u8239\u53ea\u68c0\u6d4b\u4efb\u52a1</code></p> <ul> <li>Ship Detection Algorithm</li> <li>Optimized Method</li> <li>FPGA Architecture</li> <li>Device: Xilinx XQR5VFX130</li> </ul> Hardware-oriented ClusteringData-buffer Cycle <p>Each processing engine can implement a sort of image operation functions, which has a similar regularity of operation and accessing.</p> <p>The DBC provides a method to analyze the memory occupancy period. The memory reuse rules for intermediate are designed according to DBC.</p> <p> </p>  Workflow of the proposed optimization mapping method  <p><code>Hardware-oriented Clustering</code></p> <p>regularity of data reading and writing -&gt; clustering processing engine </p> <p>According to Overlap/Non-overlap &amp; Whole/Partial Pixals.</p> <p> </p>  Data accessing and seeking addressing regularity  <p>Tip</p> <p>\u8986\u76d6\u7684\u6570\u636e\u53ef\u4ee5\u4fdd\u7559\u4f7f\u7528\uff0c\u907f\u514d\u91cd\u590d\u8bfb\u53d6\uff1b\u5e76\u4e0d\u662f\u6240\u6709\u64cd\u4f5c\u90fd\u9700\u8981\u7a97\u53e3\u4e2d\u7684\u6240\u6709\u50cf\u7d20\u503c.</p> <p>5 categories:</p> <ul> <li>overlaping and whole data</li> <li>non-overlaping and whole data</li> <li>non-overlaping and partial data</li> <li>overlaping and adjacency(partial) data</li> <li>single pixel traversal</li> </ul> <p> </p>  Example for 5 categories  <p>Compare clustering method with traditional method.</p> 7 processing modules3 processing modules <p>  Traditional method  </p> <p>  Clustering method  </p> <p>Tip</p> <p>\u8be5\u65b9\u6cd5\u4e0d\u6539\u53d8\u6d41\u7a0b\uff0c\u53ea\u8d77\u5230\u590d\u7528\u6a21\u5757\uff0c\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u7684\u7528\u5904.</p> <p><code>Data-buffer Cycle</code></p> <p>We assume that the intermediate data are generated in the Nth processing step and used by the Mth processing step. Then, the DBC is defined as M-N. The length of the DBC is equivalent to the total time that the data takes up in memory.</p> <p>3 categories:</p> <ul> <li>DBC = 1 -&gt; short</li> <li>DBC &gt; 60% of all steps -&gt; long</li> <li>remain -&gt; median</li> </ul> <p>DBC(D1) = 4  generated in step1, used in step 5</p> <p>  Example of processing algorithm  </p> <p><code>implementation</code></p> <p> </p>  Architecture overview  <p>\u6bcf\u4e2aPEU(Processing Engine Unit)\u7531\u76f8\u540c\u7684\u51e0\u90e8\u5206\u6784\u6210:</p> <ul> <li>operation module: \u6267\u884c\u8ba1\u7b97\u64cd\u4f5c(clutering\u540e\u7684\u64cd\u4f5c)</li> <li>schedule module: \u6839\u636etiming\u5224\u65ad\u8be5\u8fdb\u884c\u4ec0\u4e48\u64cd\u4f5c\uff0c\u4e0b\u53d1\u7ed9operation module</li> <li>resister control module: \u53d8\u66f4\u53c2\u6570\uff0cread enable\u7b49\u4fe1\u53f7</li> <li>monitor module: \u7528\u4e8e\u8c03\u8bd5\u89c2\u5bdf</li> </ul> overlaping and wholenon-overlaping and wholenon-overlaping and partialoverlaping and adjacency(partial)single pixel traversal <p>  Overlapping and overall data engine  </p> <p>  Nonoverlapping and overall data engine  </p> <p>  Nonoverlapping and partial data engine  </p> <p>  Overlapping and adjacency data engine  </p> <p>  Single pixel traversal engine  </p> <p>MU(Memory Unit) is divided into 2 part: On-chip &amp; Off-chip</p> <p> </p>  MU Statistics  <p><code>result</code></p> <p> </p>  Detection example  <p>Compare with other devices using the same algorithm. Using OPENCV to add some image-processing function.</p> CPU DSP FPGA FPGA FPGA DELL workstation(E5-2650, 8G RAM) TS201(800MHz) pipeline HLS proposed method <p> </p>  Resource consumption results of different devices  <p><code>real-time performance</code></p> <p> </p>  Resource consumption results of different devices  <p>\u5f53\\(\\tau = \\frac{T_{Tr}}{Max(T_{Im},T_{Dp})} &gt; 1\\)\u65f6\uff0c\u53ef\u5b9e\u73b0real-time performance<sup>1</sup>.</p> <ul> <li> <p>\\(T_{Tr}\\):\u539f\u59cb\u6570\u636e\u83b7\u53d6\u65f6\u95f4</p> </li> <li> <p>\\(T_{Im}\\):image focusing\u65f6\u95f4</p> </li> <li> <p>\\(T_{Dp}\\):image detection\u65f6\u95f4</p> </li> </ul>"},{"location":"Paper-Reading/On_orbit-Accelerator/#question_1","title":"Question","text":"<p>\u8fd9\u51e0\u53e5\u8bdd\u6ca1\u8bf4\u6e05\u695a\uff0cshort \u548c long \u90fd\u662f\u7528\u4e00\u6837\u7684\u5b58\u50a8\u5355\u5143?</p> <p>The short DBC data are arranged to use the same storage. The medium DBC data can reuse memory in the case that the algorithm processing flows arenot affected. The standalone storage is reserved for the long DBC data.</p> <p>What is FPGA with pipeline and HLS?</p>"},{"location":"Paper-Reading/On_orbit-Accelerator/#an-fpga-based-hybrid-neural-network-accelerator-for-embedded-satellite-image-classification","title":"<code>An FPGA-Based Hybrid Neural Network Accelerator for Embedded Satellite Image Classification</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#summary_3","title":"Summary","text":"<p><code>\u6df7\u5408FPGA\u7f51\u7edc\u6a21\u578b\u52a0\u901f\u5668\u67b6\u6784</code></p> <ul> <li>Application: Cloud coverage classification</li> <li>Formal Neural Network: Feature extraction</li> <li>Spiking Neural Network: Classification (Fully connect)</li> <li>Device: Xilinx Cyclone V FPGA</li> </ul> <p> </p>  Topology of hybrid network  <p><code>formal domain</code></p> <p>\u5177\u6709\u9ad8\u5ea6\u5e76\u884c\u6027</p> Convolution layerMaxpoolong layer <p>About filter-wise adder</p> <p>filter-wise filter\u53ef\u4ee5\u540c\u65f6\u5bf9\u4e0d\u540cchannel\u7684feature map\u8fdb\u884c\u8ba1\u7b97\uff0c\u9700\u8981\u540c\u65f6\u5b58\u50a8\\(C_{in} \\times C_{out}\\)\u4e2aconv kernal.</p> <p>  Schematic representation of the convolution layer architecture  </p> <p>Maxpooling consists in dividing the 2D input in patches, extracting the maximum value of each patch, and building a sub-sampled version of the input with only those maximum values.</p> <p><code>interface between formal and spiking domain</code></p> <ul> <li> <p>Data flow: Flattening Module -&gt; Spiking Generation Cell</p> </li> <li> <p>Spikes are represented by 2 co-occurrent signal: o_spike &amp; o_addr.</p> </li> </ul> <p><code>spiking domain</code></p> <p>Integrating weighted input events, triggering an output event when the accumulator overpasses a threshold, and resetting the accumulator when emitting an output event.</p> <p> </p>  Schematic representation of classification modules  <p>10 * IF_1 &amp; 2 * IF_2 &amp; Terminate Delta Module -&gt; count spikes and enact classification outcome.</p> <p>Tip</p> <p>\u53ea\u6709spike generation module\u540e\u7684spike\u9700\u8981o_addr\uff0c\u5176\u4ed6\u76f4\u63a5\u901a\u8fc7neuron\u95f4\u7684\u8fde\u63a5\u4f20\u9012\u4fe1\u606f.</p> <p><code>architecture</code></p> <p> </p>  Schematic representation of the full system architecture  <ol> <li>ARM A9\u8bfb\u53d6\u56fe\u7247\u5e76\u5206\u5272\uff0c\u50a8\u5b58\u5728DDR\u4e2d.</li> <li>scheduler\u6839\u636eimage pitch\u5730\u5740\u8bbe\u7f6eDMA\uff0c\u5e76\u89e6\u53d1CNN accelerator.</li> <li>scheduler\u5c06\u7ed3\u679c\u5b58\u56deDDR\u4e2d.</li> </ol> <p><code>result</code></p> <p> </p>  Results of our Hybrid Architecture and its formal counterpart"},{"location":"Paper-Reading/On_orbit-Accelerator/#question_2","title":"Question","text":"<p>\u4e3a\u4ec0\u4e48\u6709/255?period\u8303\u56f4\u4e0d\u662f\\([\\frac{1}{MaxFreq},\\frac{1}{MinFreq}]\\).</p> <p>  Calculate the period for SNN  </p>"},{"location":"Paper-Reading/On_orbit-Accelerator/#fpga-based-satellite-image-classification-for-water-bodies-detection","title":"<code>FPGA-based Satellite Image Classification for Water Bodies Detection</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#summary_4","title":"Summary","text":"<p><code>\u4e3a\u706b\u60c5\u5904\u7406\u63d0\u4f9b\u670d\u52a1\u7684\u6c34\u4f53\u63a2\u6d4b\u5206\u7c7b\u52a0\u901f</code></p> <ul> <li>Application: Water body classification</li> <li>Algorithm: Decision tree and Minimun distance</li> <li>Device: Zynq UltraScale+ FPGA</li> </ul> OriginResult <p>  Original RGB Image  </p> <p>  Output of Decision Tree Classifier  </p> <p>If the project to be implemented requires lot of calculations and they have the possibility to fully be parallelized, FPGA benefits of its high number of Digital Signal Processors to accelerate these calculations. If a more sequential approach is taken, CPU will perform the same result faster.</p> <p>decision tree\u65f6CPU\u6bd4FPGA\u5feb400\u500d\uff0cminimun distance\u65f6FPGA\u6bd4CPU\u5feb4.5\u500d.</p>"},{"location":"Paper-Reading/On_orbit-Accelerator/#fpga-implementation-of-a-hardware-optimized-automatic-target-detection-and-classification-algorithm-for-hyperspectral-image-analysis","title":"<code>FPGA Implementation of a Hardware Optimized Automatic Target Detection and Classification Algorithm for Hyperspectral Image Analysis</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#summary_5","title":"Summary","text":"<p><code>FPGA\u5b9e\u73b0\u8d85\u5149\u8c31\u56fe\u50cf\u81ea\u52a8\u8bc6\u522b\u4e0e\u5206\u7c7b</code></p> <ul> <li>Application: ATDCA on HSI (Hyper Spectral Image)</li> <li>GS orthogonalization</li> <li>Device: Zynq Ultrascale + ZCU106 System</li> <li>Advantage: Execution time and power consumption</li> </ul>"},{"location":"Paper-Reading/On_orbit-Accelerator/#an-fpga-based-hardware-accelerator-for-cnns-inference-on-board-satellites-benchmarking-with-myriad-2-based-solution-for-the-cloudscout-case-study","title":"<code>An FPGA-Based Hardware Accelerator for CNNs Inference on Board Satellites: Benchmarking with Myriad 2-Based Solution for the CloudScout Case Study</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#summary_6","title":"Summary","text":"<p><code>\u57fa\u4e8eCloudScout\u7684CNN\u52a0\u901f\u5668</code></p> <ul> <li>Application: Cloud detection on hyperspectral images</li> <li>Study Case: CloudScout</li> <li>Device: Zynq Ultrascale+ ZCU106</li> <li>Counterpart: Intel Mariad 2 VPU</li> </ul> <p>Why not Myriad 2?</p> <ul> <li> Achieve good timing performance and reduced power consumption.</li> <li> Not designed for space environment (lacking in radiation-tolerant).</li> <li> Not customizable. (space-oriented communication protocol, operative system, redundancy structures ...).   brief LEO mission only</li> </ul> <p><code>accelerator categories</code></p> <p>This framework uses single PU architecture.</p> Single Processing Unit architectureStreamline architectureMultiple Shared Processing Units architecture <ul> <li>Compute operations of all the layers.</li> <li> Reduce resource exploitation.</li> <li> Increased latency and power consumption. (Data access)</li> <li>Tiling or the use of additional caches are some of the techniques to reduce the amount of data to store.</li> </ul> <ul> <li>Consists of a cascade of several blocks, one per layer of the network.</li> <li> Reducing latency and power consumption. (only the input image shall be read)</li> <li> Exploiting pipeline.</li> <li> Allows optimizing the architecture of specific layers at the price of an increased latency.</li> <li> Overhead in terms of hardware resources.</li> </ul> <ul> <li>Exploiting several PUs shared by layers with similar computation requirements.</li> <li> Better utilization of hardware and an increment of throughput compared to the solution using a single PU.</li> <li> Reduced portability. (association of PUs)</li> </ul> <p><code>model quantization</code></p> <p>Network architecture as following.</p> <p> </p>  CloudScout Model Architecture  <p>\u53c2\u8003\u6587\u732e\u505a\u6cd5\uff0c\u5404\u5c42\u91cf\u5316\u4f4d\u6570\u7531\u4e0b\u5f0f\u7ed9\u51fa</p> \\[b_{in}|_{layer=q+1} = (b_{out}-b_{tr}-b_{sat})|_{layer=q}\\] <p><code>accelerator architecture</code></p> <p> </p>  CloudScout Hardware Accelerator architecture  Custom CacheShared Convolutional LayerFilter MemoryMax Pooling <ul> <li>Storing tiles of the feature maps.</li> <li>The cache can be filled while the accelerator is performing computations.</li> <li>Parameter: \\(C_{width}\\), \\(C_{depth}\\) and N_{co}.</li> <li>Memory footprint: \\(M_{fp} = C_{width} \\times C_{depth} \\times N_{co}\\)</li> </ul> <p>  Custom cache scheduling example  </p> <p>About selecting parameters</p> <p>\\(C_{depth}\\) is given by evaluating the minimum storage resources needed by each layer to start computing. \\(M_{fp}\\) should not to exceed the available FPGA on-chip memory resources.</p> <p><code>result</code></p> <p>Deploy on Zynq Ultrascale+ ZCU106 Development Board and rad-hard Xilinx Kintex Ultrascale XQRKU060</p> <p> Output as following.</p> <p> </p>  Examples of accelerator outputs  Zynq Ultrascale+ ZCU106Kintex Ultrascale XQRKU060 <p>  CloudScout characterization system  </p> <ul> <li>PS-Master-Accelerator-Slave interface\u8d1f\u8d23\u8bfb\u5199\u52a0\u901f\u5668\u72b6\u6001. \u200b* Accelerator-Master-PS-Slave interface\u8d1f\u8d23\u6570\u636e\u4f20\u9001.</li> </ul> <p>Tip</p> <p>Accelerator AXI Master\u4f7f\u7528asynchronous FIFO, \u4f7f\u6570\u636e\u8bfb\u53d6\u65f6\u949f\u9891\u7387\u72ec\u7acb\u4e8e\u52a0\u901f\u5668\u9891\u7387, \u51cf\u5c11transfer overhead times.</p> <p>  Measured results summary   Resource occupation for the entire system  </p> <ul> <li>UltraRAM -&gt; Custom Cache</li> <li>BlockRAM -&gt; Filters and AXI FIFOs</li> <li>DSP -&gt; MAC modules in the SCL and in the Fully-Connected layers</li> </ul> <p>  Measured results summary   Resource occupation for the entire system  </p> <p>Tip</p> <p>The XQRKU060 has worse timing performance with respect to the design implemented on the ZCU106 board because it belongs to an older FPGA family (Ultrascale vs. UltraScale+).</p> <p><code>benchmark</code></p> <ul> <li>fault tolerance </li> </ul> <p>Increased clock frequencies, reduced feature sizes, and reduced supply and threshold voltages, harm the fault tolerance of the circuit. FPGA\u6709\u66f4\u597d\u7684fault tolerance, \u6709\u66f4\u9ad8\u7684LET (TID\u9ad8\u5e76\u4e0d\u6784\u6210\u4f18\u52bf).</p> <p> </p>  Examples of accelerator outputs  <ul> <li>cost </li> </ul> <p>Bad environment(wide range of temperature, absence of convection, strong vibrations...) -&gt; Complex producing -&gt; High cost</p> <ul> <li>flexibility </li> </ul> <p>Allowing algorithms and functionalities to be implemented.</p> <ul> <li>developing time </li> </ul> <p>The FPGA design flow consists of various phases, such as coding, functional simulation, synthesis and implementation, and system validation.</p> <ul> <li>inference time </li> </ul> <p>rad-hard device\u9700\u8981\u989d\u5916\u7684protection logic, \u5bfc\u81f4\u9891\u7387\u964d\u4f4e\u80fd\u8017\u589e\u52a0. This framework chooses values for the configurable parameters of the accelerator with the purpose to minimize the inference time, at the cost of hardware resources and power consumption.</p> <p> </p>  Results summary for XCZU7EV, XQRKU060, and Myriad-2 VPU  <ul> <li>power consumption </li> </ul> <p>FPGAs have greater static power consumption, VPU consumes few mW in its idle state. -&gt; FPGA suitable for online task(continuous infenrence) &amp; VPU suitable for offline task(often rest in idle state).</p>"},{"location":"Paper-Reading/On_orbit-Accelerator/#question_3","title":"Question","text":"<p>Custom Cache\u7684\\(C_{width}\\)\u662f\u4e00\u4e2aAXI packet\u7684\u957f\u5ea6\uff0c\u8fd8\u662f\u4e00\u4e2adata\u7684\u5b57\u957f?</p>"},{"location":"Paper-Reading/On_orbit-Accelerator/#h-bnn-fpga-based-binarized-convolutional-neural-network-for-cloud-detection-on-satellite-payload","title":"<code>H-BNN: FPGA-based binarized convolutional neural network for cloud detection on satellite payload</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#summary_7","title":"Summary","text":"<p><code>\u57fa\u4e8eFPGA\u76841\u4f4d\u5bbd\u68c0\u6d4b\u7f51\u7edc</code></p> <ul> <li>Application: Cloud detection</li> <li>Dataset: By the Sentinel-2 satellite</li> <li>Device: Xilinx Kintex7 410T FPGA</li> </ul> <p><code>work flow</code></p> <p> </p>  Cloud classification on-board satellite payload architecture  <p><code>H-BNN architecture</code></p> <p>Convolution -&gt; BinaryConvolution Activation -&gt; Sign Multiplication -&gt; Multiplexer</p> <p> </p>  Proposed FPGA-based binarized convolutional neural network  <p>2:1 multiplexer can be implemented with a simple logic gate.</p> <p> </p>  Binary convolution in hardware design  <p>Tip</p> <p>BNs reduce sensitivity to minor input changes, preventing vanishing gradient problem, and accelerating the training process.</p> <p><code>system architecture</code></p> <p> </p>  Hardware implementation architecute of proposed H-BNN  <p>\u5e76\u4e0d\u662f\u6240\u6709\u6570\u636e\u90fd\u662f1-bitwise, \u4ec5\u5728convolution\u8fdb\u884c\u7b80\u5316\uff0c\u5176\u4ed6\u5730\u65b9\u4ecd\u6309\u6b63\u5e38\u6570\u636e\u5904\u7406.</p>"},{"location":"Paper-Reading/On_orbit-Accelerator/#question_4","title":"Question","text":"<p>convolution layer weight\u53d8\u4e3a2\uff1a1multiplexer\u7684\u9009\u62e9\u7aef, \u600e\u4e48\u8ba1\u7b97\u68af\u5ea6\u5e76\u53cd\u5411\u4f20\u64ad.</p>"},{"location":"Paper-Reading/On_orbit-Accelerator/#literature-reviewa-survey-of-fpga-based-accelerators-for-convolutional-neural-networks","title":"(Literature Review)<code>A Survey of FPGA-based Accelerators for Convolutional Neural Networks</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#tile-grained-pipeline-architecture-for-low-latency-cnn-inference","title":"<code>Tile-Grained Pipeline Architecture for Low Latency CNN Inference</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#summary_8","title":"Summary","text":"<p>``</p> <ul> <li>Application: Cloud detection</li> <li>Dataset: By the Sentinel-2 satellite</li> <li>Device: Xilinx Kintex7 410T FPGA</li> </ul>"},{"location":"Paper-Reading/On_orbit-Accelerator/#a-high-performance-fpga-based-accelerator-for-large-scale-convolutional-neural-networks","title":"<code>A High Performance FPGA-based Accelerator for  Large-Scale Convolutional Neural Networks</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#mem-opt-a-scheduling-and-data-re-use-system-to-optimize-on-chip-memory-usage-for-cnns-on-board-fpgas","title":"<code>MEM-OPT: A Scheduling and Data Re-Use System to Optimize On-Chip Memory Usage for CNNs On-Board FPGAs</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#summary_9","title":"Summary","text":"<p><code>\u9488\u5bf9CNN\u90e8\u7f72\u7684FPGA\u5185\u5b58\u4f18\u5316</code></p> <ul> <li>Trade-off: Bandwidth and on-chip memory</li> <li>Device: Xilinx XC7Z020</li> <li>Scheduling algorithm</li> <li>Data re-use system</li> </ul>"},{"location":"Paper-Reading/On_orbit-Accelerator/#a-complete-design-flow-for-mapping-cnn-onto-embedded-fpga","title":"<code>A Complete Design Flow for Mapping CNN Onto Embedded FPGA</code>","text":""},{"location":"Paper-Reading/On_orbit-Accelerator/#literature-reviewdeep-learning-on-fpgas-past-present-and-future","title":"(Literature Review)<code>Deep Learning on FPGAs Past, Present, and Future</code>","text":"<ol> <li> <p>System can guarantee the outputs continuously in case of continuous inputs.\u00a0\u21a9</p> </li> </ol>"}]}